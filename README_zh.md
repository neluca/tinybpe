# ğŸš€tinybpe

ğŸ‘‹ **tinybpe** æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½ã€è½»é‡ã€æ•´æ´çš„**è¯­è¨€æ¨¡å‹**åˆ†è¯å™¨å’Œ **BPE** è®­ç»ƒå™¨ã€‚



## ğŸ“¦å®‰è£…

```bash
pip install tinybpe
```



## ğŸŒŸç‰¹æ€§ï¼š

- æ ¸å¿ƒç”± **C** è¯­è¨€ç²¾å¿ƒè®¾è®¡å®ç°ï¼Œå¿«é€Ÿé«˜æ•ˆ
- ä»¥ **Python** æ¨¡å—çš„å½¢å¼ä½¿ç”¨ï¼Œç®€æ´ä¼˜é›…
- æ”¯æŒ **BPE** æ¨¡å‹è®­ç»ƒå’Œå¯¼å…¥æ¨¡å‹åç»§ç»­è®­ç»ƒä»¥æ‰©å……è¯è¡¨
- å®ç°é€šç”¨çš„å­—èŠ‚çº§åˆ†è¯å™¨ï¼Œæ”¯æŒå¿«é€Ÿç¼–è§£ç å’Œæµå¼è§£ç 
- æ”¯æŒæ­£åˆ™è¡¨è¾¾å¼é¢„åˆ†è¯å’Œæ·»åŠ ç‰¹æ®Š **Token** 

- æ”¯æŒè½¬æ¢ **tiktoken** çš„æ¨¡å‹å‚æ•°
- ååˆ†å®¹æ˜“é›†æˆå’Œæ‰©å±•ï¼Œæ ¸å¿ƒé›¶ä¾èµ–



## âš¡ï¸å¿«é€Ÿå¼€å§‹

#### ğŸ“1ã€è®­ç»ƒBPEæ¨¡å‹

ä¸‹åˆ—ä»£ç æ˜¯è®­ç»ƒä¸€ä¸ªç®€å•çš„**BPE**æ¨¡å‹ï¼Œå¯¼å…¥æ–‡æœ¬æ•°æ®ï¼Œä¸å¯¹æ•°æ®åšä»»ä½•é¢„å¤„ç†ï¼Œç›´æ¥äº¤ç”±`SimpleTrainer` è®­ç»ƒå‡ºä¸€ä¸ªè¯è¡¨å¤§å°ä¸º`1000` çš„åˆ†è¯å™¨ã€‚

```python
from tinybpe import SimpleTrainer

text = open("the-old-man-and-the-sea.txt", "r", encoding="utf-8").read()  # å¯¼å…¥æ–‡æœ¬æ–‡ä»¶
trainer = SimpleTrainer(text)  # åˆ›å»ºè®­ç»ƒå™¨
vocab_size = 1000  # è¯è¡¨å¤§å°
merges_size = vocab_size - 256  # æ¨¡å‹çš„å‚æ•°å¤§å°
for _ in range(merges_size):
    pair, rank, freq = trainer.step()  # è®­ç»ƒ
    print(f"{pair} -> {rank} ({freq})")  # æ‰“å°è®­ç»ƒæ—¶çš„æ—¥å¿—

print(trainer.merges)  # æ¨¡å‹å‚æ•°ï¼Œç±»ä¼¼ä¸
print(trainer.merges_size)  # æ¨¡å‹çš„å‚æ•°å¤§å°ï¼Œè¿™é‡Œæ˜¯ 744 (1000 - 256)
trainer.save("simple")  # ä¿å­˜æ¨¡å‹æ–‡ä»¶ simple.tinymodel
```

æ³¨æ„ï¼šæ¨¡å‹çš„**è¯è¡¨**å¤§å° = **256** + æ¨¡å‹çš„**å‚æ•°**å¤§å°(**merges_size**)

è®­ç»ƒå¤æ‚çš„**BPE**æ¨¡å‹ï¼Œå¯ä»¥æŒ‰ä½ è‡ªå·±çš„éœ€æ±‚ï¼Œè®¾è®¡æ•°æ®é¢„å¤„ç†å‡½æ•°æˆ–è€…ç»§æ‰¿`SimpleTrainer`ï¼Œä¹Ÿå¯ä»¥åŠ è½½å·²æœ‰çš„å¯ç”¨çš„**tinybpe**æ¨¡å‹ï¼Œç»§ç»­è®­ç»ƒä»¥æ‰©å……è¯è¡¨ï¼Œè¯¦æƒ…å¯ä»¥å‚è€ƒğŸ“‚**examples** é‡Œé¢çš„ä¾‹å­ã€‚



#### ğŸ“2ã€åŠ è½½æ¨¡å‹åˆ›å»ºåˆ†è¯å™¨

é€šè¿‡`load_bpe_model`å°†æ¨¡å‹æ–‡ä»¶å¯¼å…¥ä¸ºåˆ†è¯å™¨æ¨¡å‹å‚æ•°ï¼Œå†ç”±æ¨¡å‹å‚æ•°åˆ›å»º `Tokenizer` å®ä¾‹ã€‚

```python
from tinybpe import Tokenizer, load_bpe_model

model = load_bpe_model("simple.tinymodel")  # å¯¼å…¥æ¨¡å‹æ–‡ä»¶
tokenizer = Tokenizer(model)  # åˆ›å»ºåˆ†è¯å™¨å®ä¾‹
s1 = "hello world, old man !"
ids = tokenizer.encode(s1)  # ç¼–ç 
print(ids)
s2 = tokenizer.decode(ids)  # è§£ç 
print(s2)
print(tokenizer.n_vocab)  # è¾“å‡ºè¯è¡¨å¤§å°
tokenizer.save_vocab("simple")  # å¯¼å‡ºè¯è¡¨æ–‡ä»¶ simple.vocab
```

`Tokenizer` æœ‰ä¸‰ä¸ªå‚æ•°ï¼Œå¦å¤–ä¸¤ä¸ªå‚æ•°åˆ†åˆ«æ˜¯ `pat_str` å’Œ `special_tokens`ï¼Œä½œç”¨å’Œ **tiktoken** ä¸€è‡´ã€‚`pat_str` æ˜¯ä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼å­—ç¬¦ä¸²ï¼Œè´Ÿè´£åˆ†è¯å™¨çš„é¢„åˆ†è¯ï¼›`special_tokens` ä¸ºæ·»åŠ çš„ç‰¹æ®Š **Tokens** å­—å…¸ã€‚è¯¦æƒ…å¯ä»¥å‚è€ƒğŸ“‚**examples** é‡Œé¢çš„ä¾‹å­ã€‚



#### ğŸ“3ã€æµå¼è§£ç 

æµå¼è§£ç å³ç”¨ä¸€ä¸ªä¸€ä¸ªçš„ **Token ID** è§£ç å‡ºå­—ç¬¦ä¸²ï¼Œé‡åˆ°ä¸è¶³ä»¥ç”¨ *unicode* è§£ç çš„å­—èŠ‚ï¼Œå†…éƒ¨ä¼šç¼“å­˜è¯¥å­—èŠ‚ï¼Œç›´åˆ°èƒ½å¤Ÿæ­£å¸¸è§£ç ä¸ºæ­¢ã€‚

```python
from tinybpe import Tokenizer, load_bpe_model

model = load_bpe_model("simple.tinymodel")
tokenizer = Tokenizer(model)

s = "hello world ä½ å¥½ä¸–ç•Œ ğŸ˜"
ids = tokenizer.encode(s)

# å­—ç¬¦ä¸²å¤„ç†å‡½æ•°
def cb_print(text):
    print(text, end="")

decode = tokenizer.stream_decode(cb_print)  # ç”Ÿæˆæµå¼è§£ç çš„è§£ç å‡½æ•°
for i in ids:
    decode(i)  # ä¸€ä¸ªä¸€ä¸ªç”¨ Token ID å»è§£ç 
```



#### ğŸ“4ã€è½¬æ¢tiktokenæ¨¡å‹

å°† **tiktoken** çš„æ¨¡å‹å‚æ•°ï¼Œä¹Ÿå°±æ˜¯å°† `mergeable_ranks` ä¿å­˜èƒ½å¤Ÿè¢« **tinybpe** æ­£å¸¸åŠ è½½çš„æ¨¡å‹æ–‡ä»¶ã€‚

```python
import tiktoken
from tinybpe import save_from_tiktoken

enc = tiktoken.get_encoding("cl100k_base")
save_from_tiktoken("cl100k_base", enc._mergeable_ranks)  # å°† tiktoken å‚æ•°ä¿å­˜ä¸º tinybpe çš„æ¨¡å‹æ–‡ä»¶
```

è¯¦æƒ…å¯ä»¥å‚è€ƒğŸ“‚**examples** é‡Œé¢ä½¿ç”¨ **tiktoken** æ¨¡å‹å‚æ•°çš„ä¾‹å­ã€‚



## âŒ›å•å…ƒæµ‹è¯•

```bash
pip install -r requirements_dev.txt
python -m pytest
```

