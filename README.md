[ä¸­æ–‡|[English](https://github.com/neluca/tinybpe/blob/main/README_en.md)]

# ğŸš€tinybpe

ğŸ‘‹ **tinybpe** æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½ã€è½»é‡ã€æ•´æ´çš„**è¯­è¨€æ¨¡å‹**åˆ†è¯å™¨å’Œ **BPE** è®­ç»ƒå™¨ã€‚

## ğŸ“¦å®‰è£…

```bash
pip install tinybpe
```

## ğŸŒŸç‰¹æ€§ï¼š

- æ ¸å¿ƒç”± **C** è¯­è¨€ç²¾å¿ƒè®¾è®¡å®ç°ï¼Œä½¿ç”¨ **AVL-Tree** ä½œä¸ºç´¢å¼•ï¼Œå¿«é€Ÿé«˜æ•ˆã€‚
- ä»¥ **Python** æ¨¡å—çš„å½¢å¼ä½¿ç”¨ï¼Œ`API`ç®€æ´ä¼˜é›…ã€‚
- æ”¯æŒ **BPE** æ¨¡å‹è®­ç»ƒå’Œå¯¼å…¥æ¨¡å‹åç»§ç»­è®­ç»ƒä»¥æ‰©å……è¯è¡¨ã€‚
- å®ç°é€šç”¨çš„å­—èŠ‚çº§åˆ†è¯å™¨ï¼Œæ”¯æŒå¿«é€Ÿç¼–è§£ç å’Œ<u>æµå¼è§£ç </u>ã€‚
- æ”¯æŒæ­£åˆ™è¡¨è¾¾å¼é¢„åˆ†è¯å’Œæ·»åŠ ç‰¹æ®Š **Token** ã€‚
- æ”¯æŒè½¬æ¢ **tiktoken** çš„æ¨¡å‹å‚æ•°ã€‚
- ååˆ†å®¹æ˜“é›†æˆå’Œæ‰©å±•ï¼Œæ ¸å¿ƒé›¶ä¾èµ–ã€‚



## âš¡ï¸å¿«é€Ÿå¼€å§‹

### ğŸ“1ã€åŸºæœ¬ä¾‹å­

è½¬æ¢ **tiktoken** çš„æ¨¡å‹å‚æ•°ï¼Œåˆ›å»º **tinybpe** åˆ†è¯å™¨ï¼Œå¹¶å¯¹æ¯” **tiktoken**ã€‚

```python
import tiktoken
from tinybpe import Tokenizer, get_from_tiktoken

tik_tokenizer = tiktoken.get_encoding("cl100k_base")
model_param = get_from_tiktoken(tik_tokenizer._mergeable_ranks)   # è½¬æ¢æ¨¡å‹å‚æ•°
tiny_tokenizer = Tokenizer(model_param)  # åˆ›å»ºtinybpeåˆ†è¯å™¨ 

text = "ğŸ‘‹ Hello, this is an example. ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªä¾‹å­ã€‚ğŸ˜"
tik_ids = tik_tokenizer.encode(text)
tiny_ids = tiny_tokenizer.encode(text)
assert tik_ids == tiny_ids

print("tiktoken: ", tik_ids)
print("tinybpe: ", tiny_ids)

tik_text = tik_tokenizer.decode(tiny_ids)
tiny_text = tiny_tokenizer.decode(tik_ids)
assert tik_text == tiny_text

print("tiktoken: ", tik_text)
print("tinybpe: ", tiny_text)

# output:
# tiktoken:  [9468, 239, 233, 22691, 11, 420, 374, 459, 3187, 13, 220, 57668, 53901, 3922, 44388, 21043, 48044, 27452, 45829, 1811, 76460, 223]
# tinybpe:  [9468, 239, 233, 22691, 11, 420, 374, 459, 3187, 13, 220, 57668, 53901, 3922, 44388, 21043, 48044, 27452, 45829, 1811, 76460, 223]
# tiktoken:  ğŸ‘‹ Hello, this is an example. ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªä¾‹å­ã€‚ğŸ˜
# tinybpe:  ğŸ‘‹ Hello, this is an example. ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªä¾‹å­ã€‚ğŸ˜
```



### ğŸ“2ã€è®­ç»ƒBPEæ¨¡å‹

ä¸‹åˆ—ä»£ç æ˜¯è®­ç»ƒä¸€ä¸ªç®€å•çš„**BPE**æ¨¡å‹ï¼Œå¯¼å…¥`<your-text-file>`çš„æ–‡æœ¬æ•°æ®ï¼Œä¸å¯¹æ•°æ®åšä»»ä½•é¢„å¤„ç†ï¼Œç›´æ¥äº¤ç”±`SimpleTrainer` è®­ç»ƒå‡ºä¸€ä¸ªè¯è¡¨å¤§å°ä¸º`1000` çš„åˆ†è¯å™¨ã€‚

```python
from tinybpe import SimpleTrainer

text = open("<your-text-file>", "r", encoding="utf-8").read()  # å¯¼å…¥æ–‡æœ¬æ–‡ä»¶
trainer = SimpleTrainer(text)  # åˆ›å»ºè®­ç»ƒå™¨
vocab_size = 1000  # è¯è¡¨å¤§å°
merges_size = vocab_size - 256  # æ¨¡å‹çš„å‚æ•°å¤§å°
for _ in range(merges_size):
    pair, rank, freq = trainer.step()  # è®­ç»ƒ
    print(f"{pair} -> {rank} ({freq})")  # æ‰“å°è®­ç»ƒæ—¶çš„æ—¥å¿—

print(trainer.merges)  # æ¨¡å‹å‚æ•°
print(trainer.merges_size)  # æ¨¡å‹çš„å‚æ•°å¤§å°ï¼Œè¿™é‡Œæ˜¯ 744 (1000 - 256)
trainer.save("simple")  # ä¿å­˜æ¨¡å‹æ–‡ä»¶ simple.tinymodel
```

æ³¨æ„ï¼šæ¨¡å‹çš„**è¯è¡¨**å¤§å° = **256** + æ¨¡å‹çš„**å‚æ•°**å¤§å°(**merges_size**)

è®­ç»ƒå¤æ‚çš„**BPE**æ¨¡å‹ï¼Œå¯ä»¥æŒ‰ä½ è‡ªå·±çš„éœ€æ±‚ï¼Œè®¾è®¡æ•°æ®é¢„å¤„ç†å‡½æ•°æˆ–è€…ç»§æ‰¿`SimpleTrainer`ï¼Œä¹Ÿå¯ä»¥åŠ è½½å·²æœ‰çš„å¯ç”¨çš„**tinybpe**æ¨¡å‹ï¼Œç»§ç»­è®­ç»ƒä»¥æ‰©å……è¯è¡¨ï¼Œè¯¦æƒ…å¯ä»¥å‚è€ƒğŸ“‚**examples** é‡Œé¢çš„ä¾‹å­ã€‚



### ğŸ“3ã€åŠ è½½æ¨¡å‹åˆ›å»ºåˆ†è¯å™¨

é€šè¿‡`load_bpe_model`å°†æ¨¡å‹æ–‡ä»¶å¯¼å…¥ä¸ºåˆ†è¯å™¨æ¨¡å‹å‚æ•°ï¼Œå†ç”±æ¨¡å‹å‚æ•°åˆ›å»º `Tokenizer` å®ä¾‹ã€‚

```python
from tinybpe import Tokenizer, load_bpe_model

model = load_bpe_model("simple.tinymodel")  # å¯¼å…¥æ¨¡å‹æ–‡ä»¶
tokenizer = Tokenizer(model)  # åˆ›å»ºåˆ†è¯å™¨å®ä¾‹
s1 = "hello world, old man !"
ids = tokenizer.encode(s1)  # ç¼–ç 
print(ids)
s2 = tokenizer.decode(ids)  # è§£ç 
print(s2)
print(tokenizer.n_vocab)  # è¾“å‡ºè¯è¡¨å¤§å°
tokenizer.save_vocab("simple")  # å¯¼å‡ºè¯è¡¨æ–‡ä»¶ simple.vocab
```

`Tokenizer` æœ‰ä¸‰ä¸ªå‚æ•°ï¼Œå¦å¤–ä¸¤ä¸ªå‚æ•°åˆ†åˆ«æ˜¯ `pat_str` å’Œ `special_tokens`ï¼Œä½œç”¨å’Œ **tiktoken** ä¸€è‡´ã€‚`pat_str` æ˜¯ä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼å­—ç¬¦ä¸²ï¼Œè´Ÿè´£åˆ†è¯å™¨çš„é¢„åˆ†è¯ï¼›`special_tokens` ä¸ºæ·»åŠ çš„ç‰¹æ®Š **Tokens** å­—å…¸ã€‚è¯¦æƒ…å¯ä»¥å‚è€ƒğŸ“‚**examples** é‡Œé¢çš„ä¾‹å­ã€‚



### ğŸ“4ã€æµå¼è§£ç 

æµå¼è§£ç å³ç”¨ä¸€ä¸ªä¸€ä¸ªçš„ **Token ID** è§£ç å‡ºå­—ç¬¦ä¸²ï¼Œé‡åˆ°ä¸è¶³ä»¥ç”¨ *unicode* è§£ç çš„å­—èŠ‚ï¼Œå†…éƒ¨ä¼šç¼“å­˜è¯¥å­—èŠ‚ï¼Œç›´åˆ°èƒ½å¤Ÿæ­£å¸¸è§£ç ä¸ºæ­¢ã€‚

```python
from tinybpe import Tokenizer, load_bpe_model

model = load_bpe_model("simple.tinymodel")
tokenizer = Tokenizer(model)

s = "hello world ä½ å¥½ä¸–ç•Œ ğŸ˜"
ids = tokenizer.encode(s)

# å­—ç¬¦ä¸²å¤„ç†å‡½æ•°
def cb_print(text):
    print(text, end="")

decode = tokenizer.stream_decode(cb_print)  # ç”Ÿæˆæµå¼è§£ç çš„è§£ç å‡½æ•°
for i in ids:
    decode(i)  # ä¸€ä¸ªä¸€ä¸ªç”¨ Token ID å»è§£ç 
```



### ğŸ“5ã€è½¬æ¢tiktokenæ¨¡å‹

å°† **tiktoken** çš„æ¨¡å‹å‚æ•°ï¼Œä¹Ÿå°±æ˜¯å°† `mergeable_ranks` ä¿å­˜èƒ½å¤Ÿè¢« **tinybpe** æ­£å¸¸åŠ è½½çš„æ¨¡å‹æ–‡ä»¶ã€‚

```python
import tiktoken
from tinybpe import save_from_tiktoken

enc = tiktoken.get_encoding("cl100k_base")
save_from_tiktoken("cl100k_base", enc._mergeable_ranks)  # å°† tiktoken å‚æ•°ä¿å­˜ä¸º tinybpe çš„æ¨¡å‹æ–‡ä»¶
```

è¯¦æƒ…å¯ä»¥å‚è€ƒğŸ“‚**examples** é‡Œé¢ä½¿ç”¨ **tiktoken** æ¨¡å‹å‚æ•°çš„ä¾‹å­ã€‚



## ğŸš§è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ï¼Œå¦‚æœæ‚¨å‘ç°äº† **bug** æˆ–è€…æœ‰ä»»ä½•å»ºè®®å’Œæ”¹è¿›æ„è§ï¼Œæ¬¢è¿å¼€å¯ä¸€ä¸ª **issue** æ¥è®¨è®ºï¼Œå¦‚æœéœ€è¦å¾€ä»£ç ä¸­åŠ å…¥æ‚¨çš„åˆ›æ„ï¼Œæˆ–è€…ä¿®æ”¹ä¸€ä¸ª **bug**ï¼Œæ¬¢è¿æäº¤ä¸€ä¸ª **pull request**ã€‚

## ğŸ¤å‚è€ƒ

[minbpe](https://github.com/karpathy/minbpe)

[tiktoken](https://github.com/openai/tiktoken)

## âŒ›å•å…ƒæµ‹è¯•

```bash
pip install -r requirements_dev.txt
python -m pytest
```

