[ä¸­æ–‡|[English](https://github.com/neluca/tinybpe/blob/main/README_en.md)]

# ğŸš€tinybpe

[![build](https://github.com/neluca/tinybpe/workflows/build/badge.svg)](https://github.com/neluca/tinybpe/actions/workflows/python-package.yml)
[![wheels](https://github.com/neluca/tinybpe/workflows/wheels/badge.svg)](https://github.com/neluca/tinybpe/actions/workflows/wheels.yml)
[![lint](https://github.com/neluca/tinybpe/workflows/lint/badge.svg)](https://github.com/neluca/tinybpe/actions/workflows/lint.yml)
[![coverage](https://codecov.io/gh/neluca/tinybpe/branch/main/graph/badge.svg)](https://codecov.io/gh/neluca/tinybpe)
[![support-version](https://img.shields.io/pypi/pyversions/tinybpe)](https://pypi.org/project/tinybpe/)
[![license](https://img.shields.io/github/license/neluca/tinybpe)](https://github.com/neluca/tinybpe/blob/main/LICENSE)

ğŸ‘‹ **TinyBPE** æ˜¯ä¸€ä¸ªåŒ…å«é«˜æ€§èƒ½ã€è½»é‡ã€æ•´æ´çš„è¯­è¨€æ¨¡å‹åˆ†è¯å™¨å’ŒåŸºæœ¬çš„ **BPE** æ¨¡å‹è®­ç»ƒå™¨çš„ **CPython** æ‰©å±•ã€‚

## ğŸ“¦å®‰è£…

```bash
pip install tinybpe
```

## ğŸŒŸç‰¹æ€§ï¼š

- æ ¸å¿ƒç”± **C** è¯­è¨€ç²¾å¿ƒè®¾è®¡å®ç°ï¼Œä½¿ç”¨ **AVL-Tree** ä½œä¸ºç´¢å¼•ï¼Œå¿«é€Ÿé«˜æ•ˆã€‚
- ä»¥ **Python** æ¨¡å—çš„å½¢å¼ä½¿ç”¨ï¼Œ`API`ç®€æ´ä¼˜é›…ã€‚
- æ”¯æŒ **BPE** æ¨¡å‹è®­ç»ƒå’Œå¯¼å…¥æ¨¡å‹åç»§ç»­è®­ç»ƒä»¥æ‰©å……è¯è¡¨ã€‚
- å®ç°é€šç”¨çš„å­—èŠ‚çº§åˆ†è¯å™¨ï¼Œæ”¯æŒå¿«é€Ÿç¼–è§£ç å’Œ<u>æµå¼è§£ç </u>ã€‚
- æ”¯æŒæ­£åˆ™è¡¨è¾¾å¼é¢„åˆ†è¯å’Œæ·»åŠ ç‰¹æ®Š **Token** ã€‚
- æ”¯æŒè½¬æ¢ [tiktoken](https://github.com/openai/tiktoken) çš„æ¨¡å‹å‚æ•°ã€‚
- ååˆ†å®¹æ˜“é›†æˆå’Œæ‰©å±•ï¼Œé«˜å¯å®šåˆ¶ï¼Œæ ¸å¿ƒé›¶ä¾èµ–ã€‚



## âš¡ï¸å¿«é€Ÿå¼€å§‹

### ğŸ“1ã€åŸºæœ¬ä¾‹å­

è½¬æ¢ **tiktoken** çš„æ¨¡å‹å‚æ•°ï¼Œåˆ›å»º **tinybpe** åˆ†è¯å™¨ï¼Œå¹¶å¯¹æ¯” **tiktoken**ã€‚

```python
import tiktoken
from tinybpe import Tokenizer, get_from_tiktoken

tik_tokenizer = tiktoken.get_encoding("cl100k_base")
model_param = get_from_tiktoken(tik_tokenizer._mergeable_ranks)   # è½¬æ¢æ¨¡å‹å‚æ•°
tiny_tokenizer = Tokenizer(model_param)  # åˆ›å»ºtinybpeåˆ†è¯å™¨ 

text = "ğŸ‘‹ Hello, this is an example. ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªä¾‹å­ã€‚ğŸ˜"
tik_ids = tik_tokenizer.encode(text)
tiny_ids = tiny_tokenizer.encode(text)
assert tik_ids == tiny_ids

print("tiktoken: ", tik_ids)
print("tinybpe: ", tiny_ids)

tik_text = tik_tokenizer.decode(tiny_ids)
tiny_text = tiny_tokenizer.decode(tik_ids)
assert tik_text == tiny_text

print("tiktoken: ", tik_text)
print("tinybpe: ", tiny_text)

# output:
# tiktoken:  [9468, 239, 233, 22691, 11, 420, 374, 459, 3187, 13, 220, 57668, 53901, 3922, 44388, 21043, 48044, 27452, 45829, 1811, 76460, 223]
# tinybpe:  [9468, 239, 233, 22691, 11, 420, 374, 459, 3187, 13, 220, 57668, 53901, 3922, 44388, 21043, 48044, 27452, 45829, 1811, 76460, 223]
# tiktoken:  ğŸ‘‹ Hello, this is an example. ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªä¾‹å­ã€‚ğŸ˜
# tinybpe:  ğŸ‘‹ Hello, this is an example. ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªä¾‹å­ã€‚ğŸ˜
```

### ğŸ“2ã€è®­ç»ƒBPEæ¨¡å‹

ä¸‹åˆ—ä»£ç è®­ç»ƒä¸€ä¸ªç®€å•çš„**BPE**æ¨¡å‹ï¼Œå¯¼å…¥`<your-text-file>`æ–‡ä»¶ä¸­çš„æ–‡æœ¬æ•°æ®ï¼Œä¸å¯¹æ•°æ®åšä»»ä½•é¢„å¤„ç†ï¼Œç›´æ¥äº¤ç”±`SimpleTrainer` ï¼Œæ‰§è¡Œ `setp()` æ–¹æ³• `744`æ¬¡ï¼Œè®­ç»ƒå‡ºä¸€ä¸ªè¯è¡¨å¤§å°ä¸º`1000` çš„åˆ†è¯å™¨ã€‚

```python
from tinybpe import SimpleTrainer

text = open("<your-text-file>", "r", encoding="utf-8").read()  # å¯¼å…¥æ–‡æœ¬æ–‡ä»¶
trainer = SimpleTrainer(text)  # åˆ›å»ºè®­ç»ƒå™¨
vocab_size = 1000  # è¯è¡¨å¤§å°
merges_size = vocab_size - 256  # æ¨¡å‹çš„å‚æ•°å¤§å°
for _ in range(merges_size):
    pair, rank, freq = trainer.step()  # è®­ç»ƒ
    print(f"{pair} -> {rank} ({freq})")  # æ‰“å°è®­ç»ƒæ—¶çš„æ—¥å¿—

print(trainer.merges)  # æ¨¡å‹å‚æ•°
print(trainer.merges_size)  # æ¨¡å‹çš„å‚æ•°å¤§å°ï¼Œè¿™é‡Œæ˜¯ 744 (1000 - 256)
trainer.save("simple")  # ä¿å­˜æ¨¡å‹æ–‡ä»¶ simple.tinymodel
```

æ³¨æ„ï¼š

- æ¨¡å‹çš„**è¯è¡¨**å¤§å° = **256** + æ¨¡å‹çš„**å‚æ•°**å¤§å°(**merges_size**)ã€‚
- ä¸å¯¹æ•°æ®åšä»»ä½•é¢„å¤„ç†ï¼Œä¾‹å¦‚å¯¹æ–‡æœ¬å­—ç¬¦ä¸² "... hel**lo w**orld ..."ç›´æ¥è®­ç»ƒï¼Œæ¨¡å‹çš„è¯è¡¨ä¸­æœ‰æ¦‚ç‡ä¼šå‡ºç°"lo w"è¿™æ ·çš„è¯ï¼Œæ‰€ä»¥å»ºè®®é¢„å¤„ç†ä¸€ä¸‹æ–‡æœ¬æ–‡ä»¶ï¼Œå¯ä»¥å‚è€ƒ [examples/regex_trainer.py](https://github.com/neluca/tinybpe/blob/main/examples/regex_trainer.py) ã€‚
- åŠ è½½æ¨¡å‹å‚æ•°åï¼Œå¯ä»¥å¯¹æ•°æ®åšç»§ç»­è®­ç»ƒä»¥æ‰©å……è¯è¡¨ï¼Œå¯ä»¥å‚è€ƒ [examples/simple_continue_training.py](https://github.com/neluca/tinybpe/blob/main/examples/simple_continue_training.py) ã€‚

è®­ç»ƒå¤æ‚çš„**BPE**æ¨¡å‹ï¼Œå¯ä»¥æŒ‰ä½ è‡ªå·±çš„éœ€æ±‚ï¼Œè®¾è®¡æ•°æ®é¢„å¤„ç†å‡½æ•°æˆ–è€…ç»§æ‰¿`SimpleTrainer`çš„çˆ¶ç±»`bpe.Trainer`ï¼Œå®ç°å±äºä½ çš„è®­ç»ƒå™¨ï¼Œ`bpe.Trainer`ç”± **C** å®ç°çš„é«˜æ€§èƒ½ã€é«˜å¯å®šåˆ¶çš„åŸºç¡€è®­ç»ƒå™¨ï¼›ä½ ä¹Ÿå¯ä»¥åŠ è½½å·²æœ‰çš„**tinybpe**æ¨¡å‹ï¼Œåœ¨æ–‡æœ¬æ•°æ®ä¸Šç»§ç»­è®­ç»ƒï¼Œæ‰©å……ä½ è‡ªå·±çš„è¯è¡¨ã€‚

### ğŸ“3ã€åŠ è½½æ¨¡å‹å¹¶åˆ›å»ºåˆ†è¯å™¨

é€šè¿‡`load_bpe_model`å°†æ¨¡å‹æ–‡ä»¶å¯¼å…¥ä¸ºåˆ†è¯å™¨å‚æ•°ï¼Œå†ç”±æ¨¡å‹çš„å‚æ•°åˆ›å»º `Tokenizer` å®ä¾‹ã€‚

```python
from tinybpe import Tokenizer, load_bpe_model

model = load_bpe_model("simple.tinymodel")  # å¯¼å…¥æ¨¡å‹æ–‡ä»¶
tokenizer = Tokenizer(model)  # åˆ›å»ºåˆ†è¯å™¨å®ä¾‹
s1 = "hello world, old man !"
ids = tokenizer.encode(s1)  # ç¼–ç 
print(ids)
s2 = tokenizer.decode(ids)  # è§£ç 
print(s2)
print(tokenizer.n_vocab)  # è¾“å‡ºè¯è¡¨å¤§å°
tokenizer.save_vocab("simple")  # å¯¼å‡ºè¯è¡¨æ–‡ä»¶ simple.vocab
```

`Tokenizer` æœ‰ä¸‰ä¸ªå‚æ•°ï¼Œå¦å¤–ä¸¤ä¸ªå‚æ•°åˆ†åˆ«æ˜¯ `pat_str` å’Œ `special_tokens`ï¼Œä½œç”¨å’Œ **tiktoken** ä¸€è‡´ã€‚`pat_str` æ˜¯ä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼å­—ç¬¦ä¸²ï¼Œè´Ÿè´£å¯¹åˆ†è¯å™¨çš„æ–‡æœ¬å­—ç¬¦ä¸²åšé¢„å¤„ç†ï¼›`special_tokens` ä¸ºæ·»åŠ çš„ç‰¹æ®Š **Tokens** å­—å…¸ã€‚è¯¦æƒ…å¯ä»¥å‚è€ƒ [examples/regex_tokenizer.py](https://github.com/neluca/tinybpe/blob/main/examples/regex_tokenizer.py) å’Œ [examples/cl100k_tokenizer.py](https://github.com/neluca/tinybpe/blob/main/examples/cl100k_tokenizer.py) ã€‚

### ğŸ“4ã€æµå¼è§£ç 

æµå¼è§£ç ï¼Œå³ç”¨ä¸€ä¸ªä¸€ä¸ªçš„ **Token ID** è§£ç å‡ºå­—ç¬¦ä¸²ï¼Œé‡åˆ°ä¸è¶³ä»¥ç”¨ *unicode* è§£ç çš„å­—èŠ‚ï¼Œç¨‹åºå†…éƒ¨ä¼šé€‰æ‹©ç¼“å­˜è¯¥å­—èŠ‚ï¼Œç›´åˆ°èƒ½å¤Ÿæ­£å¸¸è§£ç ä¸ºæ­¢ã€‚

```python
from tinybpe import Tokenizer, load_bpe_model

model = load_bpe_model("simple.tinymodel")
tokenizer = Tokenizer(model)

s = "hello world ä½ å¥½ä¸–ç•Œ ğŸ˜"
ids = tokenizer.encode(s)
# å­—ç¬¦ä¸²å¤„ç†å‡½æ•°
def cb_print(text):
    print(text, end="")

decode = tokenizer.stream_decode(cb_print)  # ç”Ÿæˆæµå¼è§£ç çš„è§£ç å‡½æ•°
for i in ids:
    decode(i)  # ä¸€ä¸ªä¸€ä¸ªç”¨ Token ID å»è§£ç 
```

### ğŸ“5ã€è½¬æ¢tiktokenæ¨¡å‹

å°† **tiktoken** çš„æ¨¡å‹å‚æ•°ï¼Œä¹Ÿå°±æ˜¯å°† `mergeable_ranks` ä¿å­˜èƒ½å¤Ÿè¢« **tinybpe** æ­£å¸¸åŠ è½½çš„æ¨¡å‹æ–‡ä»¶ã€‚

```python
import tiktoken
from tinybpe import save_from_tiktoken

enc = tiktoken.get_encoding("cl100k_base")
save_from_tiktoken("cl100k_base", enc._mergeable_ranks)  # å°† tiktoken æ¨¡å‹å‚æ•°ä¿å­˜ä¸º tinybpe çš„æ¨¡å‹æ–‡ä»¶
```

æ‰§è¡Œä»¥ä¸Šä»£ç åï¼Œç›®å½•ä¸­ä¼šå‡ºç°ä¸€ä¸ªåä¸º ***cl100k_base.tinymodel*** çš„æ–‡ä»¶ï¼Œåªéœ€è¦åƒç¬¬ä¸‰æ­¥ä¸€æ ·å»åŠ è½½è¿™ä¸ªæ¨¡å‹ï¼Œå°±å¯ä»¥æ­£å¸¸ä½¿ç”¨äº†ï¼Œ ä¾‹å¦‚ï¼š[examples/cl100k_tokenizer.py](https://github.com/neluca/tinybpe/blob/main/examples/cl100k_tokenizer.py)ã€‚

**æ³¨æ„ï¼šåœ¨å•†ç”¨åœºæ™¯ï¼Œè½¬æ¢å…¶å®ƒåˆ†è¯å™¨çš„æ¨¡å‹éœ€è¦æ³¨æ„ç‰ˆæƒé—®é¢˜ï¼Œæ‰€ä»¥å»ºè®®è‡ªå·±è®­ç»ƒå±äºè‡ªå·±çš„åˆ†è¯å™¨æ¨¡å‹ğŸ˜›ã€‚**



## ğŸ”§è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ï¼Œå¦‚æœæ‚¨å‘ç°äº† **bug** æˆ–è€…æœ‰ä»»ä½•å»ºè®®å’Œæ”¹è¿›æ„è§ï¼Œæ¬¢è¿å¼€å¯ä¸€ä¸ª **issue** æ¥è®¨è®ºï¼›å¦‚æœéœ€è¦å¾€ä»£ç ä¸­åŠ å…¥æ‚¨çš„åˆ›æ„ï¼Œæˆ–è€…ä¿®å¤æŸä¸ª **bug**ï¼Œæ¬¢è¿æäº¤ **pull request**ã€‚

## ğŸ¤æ„Ÿè°¢

1. éå¸¸æ„Ÿè°¢ [minbpe](https://github.com/karpathy/minbpe) å¯¹BPEç®—æ³•åŸç†çš„è¯¦ç»†è§£è¯»å’Œç›¸åº”çš„ä»£ç å®ç°ã€‚

2. éå¸¸æ„Ÿè°¢ [tiktoken](https://github.com/openai/tiktoken) æä¾›çš„åˆ†è¯å™¨æ¨¡å‹ä»¥ä¾›éªŒè¯ã€‚


## âŒ›å•å…ƒæµ‹è¯•

```bash
pip install -r requirements_dev.txt
python build_setup.py build_ext
python -m pytest
```

